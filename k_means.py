# -*- coding: utf-8 -*-
"""K_means.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vYJ27-T8d91fcvFTHnSo_c67uyZZSDIN
"""

# Commented out IPython magic to ensure Python compatibility.
import cv2
from skimage import io
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
from sklearn.cluster import KMeans

from skimage.filters import gaussian
import math
# %matplotlib inline
import time
import hashlib
import scipy
import pandas as pd
from sklearn.cluster import KMeans, Birch
from sklearn.datasets.samples_generator import make_blobs
from functools import reduce

from skimage.color import convert_colorspace

!pip install imagecodecs
import imagecodecs

im0 = io.imread('/content/drive/MyDrive/Semester 01/Image proccessing/Final_project_01/0174ML0009370000105185E01_DRCL.tif')
# im0 = im0/255
im1 = gaussian(im0, sigma=7, multichannel=True)
im1 = im1[:,:,0:3]
print(type(im1))
print(im1.shape)
# im1 = rgb_hsi(im1[:,:,0:3]) # RGB to HSI space transformation
im1 = convert_colorspace(im1, 'RGB', 'HSV') # RGB to HSV space transformation
print(im1.shape)

fig = plt.figure()
ax1 = fig.add_subplot(121)
ax2 = fig.add_subplot(122)
ax1.imshow(im0)
ax2.imshow(im1)
plt.show()

fig = plt.figure()
ax1 = fig.add_subplot(131)
ax2 = fig.add_subplot(132)
ax3 = fig.add_subplot(133)
ax1.imshow(im1[:,:,0])
ax2.imshow(im1[:,:,1])
ax3.imshow(im1[:,:,2])
plt.show()

R1 = im1[:,:,0].flatten()
G1 = im1[:,:,1].flatten()
B1 = im1[:,:,2].flatten()
print(R1.shape, G1.shape, B1.shape)

# ## Decorrelation streching
# im_streched = decorrstretch(im1)
# R1 = im_streched[:,:,0].flatten()
# G1 = im_streched[:,:,1].flatten()
# B1 = im_streched[:,:,2].flatten()
# print(R1.shape, G1.shape, B1.shape)
# fig = plt.figure()
# ax1 = fig.add_subplot(121)
# ax2 = fig.add_subplot(122)
# ax1.imshow(im1)
# ax2.imshow(im_streched[:,:,0:3])
# plt.show()

# # 3D and 2D scatter plot
# fig = plt.figure()
# ax = fig.add_subplot(111, projection='3d')
# ax.scatter(R1, G1, B1)
# ax.set_xlabel("Red")
# ax.set_ylabel("Green")
# ax.set_zlabel("Blue")

# fig = plt.figure()
# ax2 = fig.add_subplot(111)
# ax2.scatter(R1, G1)
# ax2.set_xlabel("Red")
# ax2.set_ylabel("Green")

# fig = plt.figure()
# ax3 = fig.add_subplot(111)
# ax3.scatter(G1, B1)
# ax3.set_xlabel("Green")
# ax3.set_ylabel("Blue")

# # 3D and 2D scatter plot
# fig = plt.figure()
# ax = fig.add_subplot(111, projection='3d')
# ax.scatter(R1, G1, B1)
# ax.set_xlabel("Red")
# ax.set_ylabel("Green")
# ax.set_zlabel("Blue")

# fig = plt.figure()
# ax2 = fig.add_subplot(111)
# ax2.scatter(R1, G1)
# ax2.set_xlabel("Red")
# ax2.set_ylabel("Green")

# fig = plt.figure()
# ax3 = fig.add_subplot(111)
# ax3.scatter(G1, B1)
# ax3.set_xlabel("Green")
# ax3.set_ylabel("Blue")

# # # 3D and 2D scatter plot
# # fig = plt.figure()
# # ax = fig.add_subplot(111, projection='3d')
# # ax.scatter(R_streched, G_streched, B_streched)
# # ax.set_xlabel("Red")
# # ax.set_ylabel("Green")
# # ax.set_zlabel("Blue")

# # fig = plt.figure()
# # ax2 = fig.add_subplot(111)
# # ax2.scatter(R_streched, G_streched)
# # ax2.set_xlabel("Red")
# # ax2.set_ylabel("Green")

# # fig = plt.figure()
# # ax3 = fig.add_subplot(111)
# # ax3.scatter(G_streched, B_streched)
# # ax3.set_xlabel("Green")
# # ax3.set_ylabel("Blue")

# Creating a meshgrid to add position as a feature
x = np.linspace(0, 1, im1.shape[1])
y = np.linspace(0, 1, im1.shape[0])
X1, Y1 = np.meshgrid(x, y)
print(X1.shape, Y1.shape)

X1 = X1.flatten()
Y1 = Y1.flatten()
print(X1.shape, Y1.shape)

# X = np.transpose(np.array([R1, G1, B1]))
X = np.transpose(np.array([R1, G1, B1, X1, Y1]))
# X = np.transpose(np.array([R_streched, G_streched, B_streched, X1, Y1]))
print(X.shape)

# Clustering with nltk package (Kmeans with diffrent distance functions)
from nltk import cluster
from nltk.cluster import euclidean_distance, cosine_distance
from numpy import array

n_clusters = 14
# initialise the clusterer (will also assign the vectors to clusters)
clusterer = cluster.KMeansClusterer(n_clusters, cosine_distance)
Y = clusterer.cluster(X, assign_clusters=True)
Y = np.array(Y)
print(Y.shape)

# classify a new vector
# print(clusterer.classify(array([3, 3])))

# n_clusters = 14
# kmeans = KMeans(n_clusters, n_init=10, random_state=0, verbose=0).fit(X)
# Y = kmeans.predict(X)
# print(Y.shape)

layer = []
for i in range(n_clusters):
  layer_i = np.where(Y==i)
  layer_i = np.array(layer_i).T
  layer_i = np.squeeze(layer_i)
  layer.append(layer_i)
  print(layer_i.shape)

for n_cluster in range(n_clusters):
  R_i = im0[:,:,0].flatten()
  G_i = im0[:,:,1].flatten()
  B_i = im0[:,:,2].flatten()
  # print(layer[n_cluster])
  for i in np.array(layer[n_cluster]):
    i = int(i)
    R_i[i] = 255
    G_i[i] = 255
    B_i[i] = 255
  R_i = np.reshape(R_i, (im1.shape[0], -1))
  G_i = np.reshape(G_i, (im1.shape[0], -1))
  B_i = np.reshape(B_i, (im1.shape[0], -1))
  print(n_cluster, R_i.shape, G_i.shape, B_i.shape)
  im2 = np.stack((R_i, G_i, B_i), axis=2)
  fig = plt.figure()
  io.imshow(im2)

n_desired_clusters = 2
layer = merge_layers(n_desired_clusters, [[0,2,3,4,5,6,7,8,9,11,13], [1,10,12]], layer)

for n_cluster in range(n_desired_clusters):
  R_i = im0[:,:,0].flatten()
  G_i = im0[:,:,1].flatten()
  B_i = im0[:,:,2].flatten()
  # print(layer[n_cluster])
  for i in np.array(layer[n_cluster]):
    i = int(i)
    R_i[i] = 255
    G_i[i] = 255
    B_i[i] = 255
  R_i = np.reshape(R_i, (im1.shape[0], -1))
  G_i = np.reshape(G_i, (im1.shape[0], -1))
  B_i = np.reshape(B_i, (im1.shape[0], -1))
  print(n_cluster, R_i.shape, G_i.shape, B_i.shape)
  im2 = np.stack((R_i, G_i, B_i), axis=2)
  fig = plt.figure()
  io.imsave('/content/drive/MyDrive/Semester 01/Image proccessing/Final_project_01/results/0100cluster_0'+str(n_cluster+1)+'.png', im2)
  io.imshow(im2)

from scipy import ndimage

print(im2.shape)
im3 = 1.0 * (im2[:,:,2] >= 255)
print(im3.shape)
im4 = ndimage.binary_fill_holes(im3, structure=np.ones((5,5))).astype(im3.dtype)
# im4 = ndimage.binary_opening(im4, structure=np.ones((25,25))).astype(im4.dtype)
print(ndimage.binary_dilation)
fig = plt.figure()
io.imshow(1- im3)
fig = plt.figure()
io.imshow(1-im4)

mask = np.array([im4.T, im4.T, im4.T])
mask = np.transpose(mask)
print(mask.shape)

im5 = im0[:,:,0:3] * (mask > 0)

io.imshow(im5)

def rgb_hsi(im):
    # RGBarr is (x,3) array
    RGBarr = np.reshape(im, (-1, 3))
    HSIarr = np.zeros(RGBarr.shape)
    for j in range(RGBarr.shape[0]):
        r = RGBarr[j][0]/255
        g = RGBarr[j][1]/255
        b = RGBarr[j][2]/255
        #print("R:",r,"G:",g,"B:",b)

        i = 1 / 3 * (r + g + b)
        s = 1 - 3 * min(r, g, b) / (r + g + b + 0.00000001)
        if s < 0.00001:
            s = 0
        # calculate h
        numerator = 0.5 * ((r - g) + (r - b))
        denominator = math.sqrt(((r - g)**2) + ((r - b)*(g - b)))
        #print("Numerator",numerator,"Denominator",denominator)
        h = math.acos(numerator / (denominator + 0.00000001)) # in radians
        if b <= g:
            h = (h * 180 / math.pi)/360
        else:
            h = (360 - (h * 180 / math.pi))/360
        #print("H:",h,"S:",s,"I",i)
        HSIarr[j] = [h, s, i]
    HSIarr = np.reshape(HSIarr, (im.shape[0], -1, 3))
    return HSIarr

def optimalK(data, nrefs=3, maxClusters=5):
    """
    Calculates KMeans optimal K using Gap Statistic from Tibshirani, Walther, Hastie
    Params:
        data: ndarry of shape (n_samples, n_features)
        nrefs: number of sample reference datasets to create
        maxClusters: Maximum number of clusters to test for
    Returns: (gaps, optimalK)
    """
    gaps = np.zeros((len(range(1, maxClusters)),))
    resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]})
    for gap_index, k in enumerate(range(1, maxClusters)):

        # Holder for reference dispersion results
        refDisps = np.zeros(nrefs)

        # For n references, generate random sample and perform kmeans getting resulting dispersion of each loop
        for i in range(nrefs):
            
            # Create new random reference set
            randomReference = np.random.random_sample(size=data.shape)
            
            # Fit to it
            km = KMeans(k)
            km.fit(randomReference)
            
            refDisp = km.inertia_
            refDisps[i] = refDisp

        # Fit cluster to original data and create dispersion
        km = KMeans(k)
        km.fit(data)
        
        origDisp = km.inertia_

        # Calculate gap statistic
        gap = np.log(np.mean(refDisps)) - np.log(origDisp)

        # Assign this loop's gap statistic to gaps
        gaps[gap_index] = gap
        
        resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True)
        print('here')

    return (gaps.argmax() + 1, resultsdf)  # Plus 1 because index of 0 means 1 cluster is optimal, index 2 = 3 clusters are optimal

def decorrstretch(A, tol=None):
    """
    Apply decorrelation stretch to image
    Arguments:
    A   -- image in cv2/numpy.array format
    tol -- upper and lower limit of contrast stretching
    """
    
    # save the original shape
    orig_shape = A.shape
    # reshape the image
    #         B G R
    # pixel 1 .
    # pixel 2   .
    #  . . .      .
    A = A.reshape((-1,3)).astype(np.float)
    # covariance matrix of A
    cov = np.cov(A.T)
    # source and target sigma
    sigma = np.diag(np.sqrt(cov.diagonal()))
    # eigen decomposition of covariance matrix
    eigval, V = np.linalg.eig(cov)
    # stretch matrix
    S = np.diag(1/np.sqrt(eigval))
    # compute mean of each color
    mean = np.mean(A, axis=0)
    # substract the mean from image
    A -= mean
    # compute the transformation matrix
    T = reduce(np.dot, [sigma, V, S, V.T])
    # compute offset 
    offset = mean - np.dot(mean, T)
    # transform the image
    A = np.dot(A, T)
    # add the mean and offset
    A += mean + offset
    # restore original shape
    B = A.reshape(orig_shape)
    # for each color...
    for b in range(3):
        # apply contrast stretching if requested
        if tol:
            # find lower and upper limit for contrast stretching
            low, high = np.percentile(B[:,:,b], 100*tol), np.percentile(B[:,:,b], 100-100*tol)
            B[B<low] = low
            B[B>high] = high
        # ...rescale the color values to 0..255
        B[:,:,b] = 255 * (B[:,:,b] - B[:,:,b].min())/(B[:,:,b].max() - B[:,:,b].min())
    # return it as uint8 (byte) image
    return B.astype(np.uint8)

def merge_layers(n_layers, layers_indices, layer):
  # n_layers : number of desired clusters
  # layer_indices : the indices which are belong to one cluster (layer)
  # layer: all clustered  layers (it should be list with number of layers vectors)
  output = []
  output_i = np.array([])
  for i in range(n_layers):
    # print('i = ', i)
    output_i = np.array([])
    for j in layers_indices[i]:
      # print('j = ', j)
      # print(np.array(layer[j]).shape)
      output_i = np.concatenate((output_i, np.array(layer[j])), axis=0)
      # print(output_i.shape)
    output.append(output_i)
  return output



k, gapdf = optimalK(X, nrefs=3, maxClusters=15)
print('Optimal k is: ', k)

plt.plot(gapdf.clusterCount, gapdf.gap, linewidth=3)
plt.scatter(gapdf[gapdf.clusterCount == k].clusterCount, gapdf[gapdf.clusterCount == k].gap, s=250, c='r')
plt.grid(True)
plt.xlabel('Cluster Count')
plt.ylabel('Gap Value')
plt.title('Gap Values by Cluster Count')
plt.savefig('/content/drive/MyDrive/Semester 01/Image proccessing/Final_project_01/Figs/Gap', dpi=300)
plt.show()